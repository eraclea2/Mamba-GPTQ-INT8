{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use this Colab notebook to do evaluation (perplexity, throughput, model size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BAA1FIai5Hgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGGfzUBhsur6"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B08i0kVQRvu4"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "\n",
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
        "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\", quantization_config=None, output_hidden_states=False, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1SZMQh2_i8n"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ethass1w1OJo"
      },
      "outputs": [],
      "source": [
        "#save the original mamba model\n",
        "torch.save(model.state_dict(), \"mamba_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48OKsk1KAiKt"
      },
      "source": [
        "## Manually changing layers: run at most 1 cell in this section, depending on which layers you want to replace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop replaces each linear layer with a int8 linear layer. Comment out any layer types you don't want to replace"
      ],
      "metadata": {
        "id": "7UrXxr0P8men"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XWBtYnGpVcZl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "threshold=6.0\n",
        "\n",
        "# This loop replaces each linear layer with a int8 linear layer. Comment out any layer types you don't want to replace\n",
        "\n",
        "# Assuming 'model' is your pre-trained MambaForCausalLM model\n",
        "# This code modify the og mamba model, where it replaces the Linear Layer to Linear8bit\n",
        "for i, block in enumerate(model.backbone.layers):\n",
        "\n",
        "\n",
        "    # inner_project layer\n",
        "    in_proj_layer = block.mixer.in_proj\n",
        "\n",
        "    in_proj_layer_in_features = in_proj_layer.in_features\n",
        "    in_proj_layer_out_features = in_proj_layer.out_features\n",
        "    in_proj_layer_bias = in_proj_layer.bias is not None\n",
        "\n",
        "    # Create a new 8-bit precision in_proj layer\n",
        "    # Make sure to set has_fp16_weights=False for inference-focused quantization\n",
        "    new_in_proj_layer = bnb.nn.Linear8bitLt(in_proj_layer_in_features, in_proj_layer_out_features,bias=in_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.in_proj = new_in_proj_layer\n",
        "\n",
        "\n",
        "    # x_project layer\n",
        "    x_proj = block.mixer.x_proj\n",
        "\n",
        "    x_proj_layer_in_features = x_proj.in_features\n",
        "    x_proj_layer_out_features = x_proj.out_features\n",
        "    x_proj_layer_bias = x_proj.bias is not None\n",
        "\n",
        "    new_x_proj_layer = bnb.nn.Linear8bitLt(x_proj_layer_in_features, x_proj_layer_out_features,bias=x_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.x_proj = new_x_proj_layer\n",
        "\n",
        "\n",
        "    # dt_project layer\n",
        "    dt_proj_layer = block.mixer.dt_proj\n",
        "\n",
        "    dt_proj_layer_in_features = dt_proj_layer.in_features\n",
        "    dt_proj_layer_out_features = dt_proj_layer.out_features\n",
        "    dt_proj_layer_bias = dt_proj_layer.bias is not None\n",
        "\n",
        "    new_dt_proj_layer = bnb.nn.Linear8bitLt(dt_proj_layer_in_features, dt_proj_layer_out_features,bias=dt_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    block.mixer.dt_proj = new_dt_proj_layer\n",
        "\n",
        "\n",
        "\n",
        "    #out_project layer\n",
        "\n",
        "    out_proj = block.mixer.out_proj\n",
        "\n",
        "    out_proj_layer_in_features = out_proj.in_features\n",
        "    out_proj_layer_out_features = out_proj.out_features\n",
        "    out_proj_layer_bias = out_proj.bias is not None\n",
        "\n",
        "    new_out_proj_layer = bnb.nn.Linear8bitLt(out_proj_layer_in_features, out_proj_layer_out_features,bias=out_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "    block.mixer.out_proj = new_out_proj_layer\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# To load the state_dict back into the model (for inference or further adjustments):\n",
        "model.load_state_dict(torch.load(\"mamba_model.pt\"))\n",
        "\n",
        "# If your deployment environment supports it, move the model to the appropriate device\n",
        "# For example, using CUDA device 0\n",
        "bit_model = model.to('cuda:0') # This also triggers the internal quantization process in bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop replaces each linear layer with a int8 or fp4 linear layer. Comment out any layer types you don't want to replace"
      ],
      "metadata": {
        "id": "v0avGbqpGiEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "threshold=6.0\n",
        "\n",
        "# Assuming 'model' is your pre-trained MambaForCausalLM model\n",
        "for i, block in enumerate(model.backbone.layers):\n",
        "\n",
        "\n",
        "    # inner_project layer\n",
        "    in_proj_layer = block.mixer.in_proj\n",
        "\n",
        "    in_proj_layer_in_features = in_proj_layer.in_features\n",
        "    in_proj_layer_out_features = in_proj_layer.out_features\n",
        "    in_proj_layer_bias = in_proj_layer.bias is not None\n",
        "\n",
        "    # Create a new 8-bit precision in_proj layer\n",
        "    # Make sure to set has_fp16_weights=False for inference-focused quantization\n",
        "    new_in_proj_layer = bnb.nn.Linear8bitLt(in_proj_layer_in_features, in_proj_layer_out_features,bias=in_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.in_proj = new_in_proj_layer\n",
        "\n",
        "\n",
        "    # x_project layer\n",
        "    x_proj = block.mixer.x_proj\n",
        "\n",
        "    x_proj_layer_in_features = x_proj.in_features\n",
        "    x_proj_layer_out_features = x_proj.out_features\n",
        "    x_proj_layer_bias = x_proj.bias is not None\n",
        "\n",
        "    new_x_proj_layer = bnb.nn.Linear4bit(x_proj_layer_in_features, x_proj_layer_out_features,bias= x_proj_layer_bias)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.x_proj = new_x_proj_layer\n",
        "\n",
        "\n",
        "    # dt_project layer\n",
        "    dt_proj_layer = block.mixer.dt_proj\n",
        "\n",
        "    dt_proj_layer_in_features = dt_proj_layer.in_features\n",
        "    dt_proj_layer_out_features = dt_proj_layer.out_features\n",
        "    dt_proj_layer_bias = dt_proj_layer.bias is not None\n",
        "\n",
        "    new_dt_proj_layer = bnb.nn.Linear8bitLt(dt_proj_layer_in_features, dt_proj_layer_out_features,bias= dt_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    block.mixer.dt_proj = new_dt_proj_layer\n",
        "\n",
        "\n",
        "\n",
        "    #out_project layer\n",
        "\n",
        "    out_proj = block.mixer.out_proj\n",
        "\n",
        "    out_proj_layer_in_features = out_proj.in_features\n",
        "    out_proj_layer_out_features = out_proj.out_features\n",
        "    out_proj_layer_bias = out_proj.bias is not None\n",
        "\n",
        "    new_out_proj_layer = bnb.nn.Linear4bit(out_proj_layer_in_features, out_proj_layer_out_features,bias= out_proj_layer_bias)\n",
        "    block.mixer.out_proj = new_out_proj_layer\n",
        "    pass\n",
        "\n",
        "\n",
        "# To load the state_dict back into the model (for inference or further adjustments):\n",
        "model.load_state_dict(torch.load(\"mamba_model.pt\"))\n",
        "\n",
        "# If your deployment environment supports it, move the model to the appropriate device\n",
        "# For example, using CUDA device 0\n",
        "bit_model = model.to('cuda:0') # This also triggers the internal quantization process in bitsandbytes"
      ],
      "metadata": {
        "id": "Q9SaANOsGT-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Size"
      ],
      "metadata": {
        "id": "WrEkNGiTHFY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY5QAHwbsL4V"
      },
      "outputs": [],
      "source": [
        "bit_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell prints model size"
      ],
      "metadata": {
        "id": "j-im2IU8HMit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in bit_model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in bit_model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "azs4uB3eHIXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCCML3egqMw"
      },
      "source": [
        "#Throughput and perplexity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5VcHEJugsaq"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop performs perplexity/throughput benchmarks."
      ],
      "metadata": {
        "id": "AgwuX3AJ753X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCjeHTzChDJT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\"\n",
        "max_length = 1024 #bit_model.config.n_positions\n",
        "stride = 512\n",
        "seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "nlls = []\n",
        "prev_end_loc = 0\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "\n",
        "num_output_tokens = 0\n",
        "\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        logits = outputs.logits\n",
        "        generated_tokens = torch.argmax(logits, dim=-1)\n",
        "        # Count non-padding tokens in the generated sequence\n",
        "        num_output_tokens += torch.sum(generated_tokens != -100).item()\n",
        "\n",
        "\n",
        "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
        "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
        "        # to the left by 1.\n",
        "        neg_log_likelihood = outputs.loss\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "    print(\"Current:\",torch.exp(torch.stack(nlls).mean()))\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).mean())\n",
        "\n",
        "end.record()\n",
        "\n",
        "# Waits for everything to finish running\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"RESULTS:\")\n",
        "print(\"num output tokens:\", num_output_tokens)\n",
        "print(\"time in seconds:\", start.elapsed_time(end)/1000)\n",
        "print(\"output tokens per second\", num_output_tokens/(start.elapsed_time(end)/1000))\n",
        "print(\"perplexity\", ppl)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}