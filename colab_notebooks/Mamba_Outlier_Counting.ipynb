{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Use this Colab notebook to do outlier counting for hidden states. To use this notebook, run each cell sequentially, following any instructions that exist\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BAA1FIai5Hgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGGfzUBhsur6",
        "outputId": "e9b090bd-603b-4b9f-adc4-f6162eb17f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B08i0kVQRvu4"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "\n",
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
        "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\", quantization_config=None, output_hidden_states=True, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1SZMQh2_i8n"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ethass1w1OJo"
      },
      "outputs": [],
      "source": [
        "#save the original mamba model\n",
        "torch.save(model.state_dict(), \"mamba_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48OKsk1KAiKt"
      },
      "source": [
        "## Manually changing each layer to Linear8bit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop replaces each linear layer with a int8 linear layer. Comment out any layer types you don't want to replace"
      ],
      "metadata": {
        "id": "7UrXxr0P8men"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWBtYnGpVcZl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "threshold=6.0\n",
        "\n",
        "# This loop replaces each linear layer with a int8 linear layer. Comment out any layer types you don't want to replace\n",
        "\n",
        "# Assuming 'model' is your pre-trained MambaForCausalLM model\n",
        "# This code modify the og mamba model, where it replaces the Linear Layer to Linear8bit\n",
        "for i, block in enumerate(model.backbone.layers):\n",
        "\n",
        "\n",
        "    # inner_project layer\n",
        "    in_proj_layer = block.mixer.in_proj\n",
        "\n",
        "    in_proj_layer_in_features = in_proj_layer.in_features\n",
        "    in_proj_layer_out_features = in_proj_layer.out_features\n",
        "    in_proj_layer_bias = in_proj_layer.bias is not None\n",
        "\n",
        "    # Create a new 8-bit precision in_proj layer\n",
        "    # Make sure to set has_fp16_weights=False for inference-focused quantization\n",
        "    new_in_proj_layer = bnb.nn.Linear8bitLt(in_proj_layer_in_features, in_proj_layer_out_features,bias=in_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.in_proj = new_in_proj_layer\n",
        "\n",
        "\n",
        "    # x_project layer\n",
        "    x_proj = block.mixer.x_proj\n",
        "\n",
        "    x_proj_layer_in_features = x_proj.in_features\n",
        "    x_proj_layer_out_features = x_proj.out_features\n",
        "    x_proj_layer_bias = x_proj.bias is not None\n",
        "\n",
        "    new_x_proj_layer = bnb.nn.Linear4bit(x_proj_layer_in_features, x_proj_layer_out_features,bias= x_proj_layer_bias)\n",
        "\n",
        "    # Replace the existing in_proj layer with the new one\n",
        "    block.mixer.x_proj = new_x_proj_layer\n",
        "\n",
        "\n",
        "    # dt_project layer\n",
        "    dt_proj_layer = block.mixer.dt_proj\n",
        "\n",
        "    dt_proj_layer_in_features = dt_proj_layer.in_features\n",
        "    dt_proj_layer_out_features = dt_proj_layer.out_features\n",
        "    dt_proj_layer_bias = dt_proj_layer.bias is not None\n",
        "\n",
        "    new_dt_proj_layer = bnb.nn.Linear8bitLt(dt_proj_layer_in_features, dt_proj_layer_out_features,bias= dt_proj_layer_bias, has_fp16_weights=False, threshold=threshold)\n",
        "\n",
        "    block.mixer.dt_proj = new_dt_proj_layer\n",
        "\n",
        "\n",
        "\n",
        "    #out_project layer\n",
        "\n",
        "    out_proj = block.mixer.out_proj\n",
        "\n",
        "    out_proj_layer_in_features = out_proj.in_features\n",
        "    out_proj_layer_out_features = out_proj.out_features\n",
        "    out_proj_layer_bias = out_proj.bias is not None\n",
        "\n",
        "    new_out_proj_layer = bnb.nn.Linear4bit(out_proj_layer_in_features, out_proj_layer_out_features,bias= out_proj_layer_bias)\n",
        "    block.mixer.out_proj = new_out_proj_layer\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# To load the state_dict back into the model (for inference or further adjustments):\n",
        "model.load_state_dict(torch.load(\"mamba_model.pt\"))\n",
        "\n",
        "# If your deployment environment supports it, move the model to the appropriate device\n",
        "# For example, using CUDA device 0\n",
        "bit_model = model.to('cuda:0') # This also triggers the internal quantization process in bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY5QAHwbsL4V",
        "outputId": "6ffa05c5-46c1-414a-dec3-a9fd97c26efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MambaForCausalLM(\n",
              "  (backbone): MambaModel(\n",
              "    (embeddings): Embedding(50280, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-47): 48 x MambaBlock(\n",
              "        (norm): MambaRMSNorm()\n",
              "        (mixer): MambaMixer(\n",
              "          (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
              "          (act): SiLU()\n",
              "          (in_proj): Linear8bitLt(in_features=1024, out_features=4096, bias=False)\n",
              "          (x_proj): Linear4bit(in_features=2048, out_features=96, bias=False)\n",
              "          (dt_proj): Linear8bitLt(in_features=64, out_features=2048, bias=True)\n",
              "          (out_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm_f): MambaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "bit_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCCML3egqMw"
      },
      "source": [
        "#Outlier Counting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5VcHEJugsaq"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop prints the outlier stats for each hidden state, for each iteration of the perplexity testing loop"
      ],
      "metadata": {
        "id": "AgwuX3AJ753X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCjeHTzChDJT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\"\n",
        "max_length = 1024 #bit_model.config.n_positions\n",
        "stride = 512\n",
        "seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "nlls = []\n",
        "prev_end_loc = 0\n",
        "\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "        hidden_states = outputs.hidden_states\n",
        "        for index, layers in enumerate(outputs.hidden_states):\n",
        "          print(\"----------\")\n",
        "          print(\"layer number:\",index)\n",
        "          layer_weight = layers.detach().cpu().numpy()\n",
        "          print(layer_weight.shape)\n",
        "          mean = layer_weight.mean()\n",
        "          std = layer_weight.std()\n",
        "          num_outliers = 0\n",
        "          num_cols_with_outliers = 0\n",
        "          outlier_rows = set()\n",
        "          outlier_cols = set()\n",
        "          for i in range(1024):\n",
        "            # num_outliers_in_ith = 0\n",
        "            for j in range(1024):\n",
        "              if abs(layer_weight[0][i][j] - mean) > 6.0 * std:\n",
        "                num_outliers += 1\n",
        "                if i not in outlier_rows:\n",
        "                  outlier_rows.add(i)\n",
        "                if j not in outlier_cols:\n",
        "                  outlier_cols.add(j)\n",
        "          print(\"num_outliers\", num_outliers)\n",
        "          print(\"num_rows_with_outliers\", len(outlier_rows))\n",
        "          print(\"num_cols_with_outliers\", len(outlier_cols))\n",
        "          print(\"mean: \", layer_weight.mean(), \"std: \", layer_weight.std())\n",
        "          print(\"max: \", layer_weight.max(), \"min: \", layer_weight.min())\n",
        "          print('---------')\n",
        "\n",
        "\n",
        "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
        "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
        "        # to the left by 1.\n",
        "        neg_log_likelihood = outputs.loss\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "    print(\"Current:\",torch.exp(torch.stack(nlls).mean()))\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).mean())\n",
        "\n",
        "print(\"perplexity\", ppl)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}